<!doctype html><html lang=en><head><title>AI-Assisted Trader - Introduction (Part 1)</title><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.780dc7a6fc8e42bf9fef490724bc6cbc57e759a0db2df60fcd517f9f667085ed.css integrity="sha256-eA3HpvyOQr+f70kHJLxsvFfnWaDbLfYPzVF/n2Zwhe0="><link rel=icon type=image/png href=/images/site/favicon.svg><meta property="og:title" content="Steven's Portfolio"><meta property="og:type" content="website"><meta property="og:description" content="Portfolio and personal blog of Steven Lasch."><meta property="og:image" content="/images/author/me.jpg"><meta property="og:url" content="https://slasch-website.netlify.app"><meta name=description content="Is it possible to use reinforcement learning to perform stock trading? Find out how in this mini-series."><script integrity="sha256-DO4ugzEwhTW1Id1UIWn0gUJWaebCYOypeTit6LW4QB4=">let theme=localStorage.getItem("theme-scheme")||localStorage.getItem("darkmode:color-scheme")||"light";theme==="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?theme="dark":theme="light"),document.documentElement.setAttribute("data-theme",theme)</script></head><body class="type-posts kind-page" data-bs-spy=scroll data-bs-target=#TableOfContents data-bs-offset=80><div class="container-fluid bg-secondary wrapper"><nav class="navbar navbar-expand-xl top-navbar shadow" id=top-navbar><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button>
<i data-feather=sidebar></i>
</button>
<a class=navbar-brand href=/><img src=/images/site/main-logo.svg id=logo alt=Logo>
Steven Lasch Portfolio</a>
<button class="navbar-toggler navbar-light" id=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#top-nav-items aria-label=menu>
<i data-feather=menu></i></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="nav navbar-nav ms-auto"><li class=nav-item><a class=nav-link href=/#home>‡¶π‡ßã‡¶Æ</a></li><li class=nav-item><a class=nav-link href=/#about>About</a></li><li class=nav-item><a class=nav-link href=/#skills>Skills</a></li><li class=nav-item><a class=nav-link href=/#experiences>Work Experience</a></li><li class=nav-item><a class=nav-link href=/#education>Education</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>‡¶Ü‡¶∞‡ßã</a><div class=dropdown-menu aria-labelledby=navbarDropdown><a class=dropdown-item href=/#projects>Projects</a>
<a class=dropdown-item href=/#publications>Publications</a>
<a class=dropdown-item href=/#featured-posts>Featured Posts</a></div></li><div id=top-navbar-divider></div><li class=nav-item><a class=nav-link id=blog-link href=/posts>‡¶™‡ßã‡¶∑‡ßç‡¶ü ‡¶∏‡¶Æ‡ßÇ‡¶π</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=theme-icon src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=theme-icon src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/main-logo.svg class=d-none id=main-logo alt=Logo>
<img src=/images/site/main-logo.svg class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder="‡¶Ö‡¶®‡ßÅ‡¶∏‡¶®‡ßç‡¶ß‡¶æ‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®" data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts/ data-filter=all>‡¶™‡ßã‡¶∑‡ßç‡¶ü ‡¶∏‡¶Æ‡ßÇ‡¶π</a></li><div class=subtree><li><a class=list-link href=/posts/introduction/ title=Introduction>Introduction</a></li><li><i data-feather=minus-circle></i><a class="active list-link" href=/posts/ai-trader/> AI Stock Trader Mini-Series</a><ul class=active><li><a class="active list-link" href=/posts/ai-trader/part-1/ title="AI Trader Pt. 1">AI Trader Pt. 1</a></li><li><a class=list-link href=/posts/ai-trader/part-2/ title="AI Trader Pt. 2">AI Trader Pt. 2</a></li><li><a class=list-link href=/posts/ai-trader/part-3/ title="AI Trader Pt. 3">AI Trader Pt. 3</a></li></ul></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/category/> Category</a><ul><li><i data-feather=plus-circle></i><a class=list-link href=/posts/category/sub-category/> Sub-Category</a><ul><li><a class=list-link href=/posts/category/sub-category/rich-content/ title="Rich Content">Rich Content</a></li></ul></li></ul></li><li><a class=list-link href=/posts/markdown-sample/ title="Markdown Sample">Markdown Sample</a></li><li><a class=list-link href=/posts/shortcodes/ title="Shortcodes Sample">Shortcodes Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ms-auto align-self-lg-center"><img class=rounded-circle src=/images/author/me_hu_cdbce0c24b9941d9.jpg alt="Author Image"><h5 class=author-name>Steven Lasch</h5><p class=text-muted>Wednesday, November 27, 2024 | 3 ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü</p></div><div class=title><h1>AI-Assisted Trader - Introduction (Part 1)</h1></div><div class=tags><ul style=padding-left:0></ul></div><div class=post-content id=post-content><h2 id=tldr>TLDR</h2><blockquote><p>In this post, I will cover concepts in <strong>reinforcement learning (RL)</strong>, specifically Q-Learning, applied to the context of maximizing returns in a simulated stock market environment.</p></blockquote><p>Enjoy!üôÇ</p><h2 id=q-learning>Q-Learning</h2><p>As briefly described above, <a href=https://en.wikipedia.org/wiki/Q-learning target=blank>Q-Learning</a> is an RL technique that learns the optimal strategy (called a <strong>policy</strong> in RL) from three distinct elements: the <strong>action space</strong>, the <strong>state space</strong>, and a <strong>reward function</strong>. Interestingly, the &ldquo;Q&rdquo; in Q-Learning comes from the act of focusing on the <em>quality</em> of each action, and choosing the best action.</p><p>In its simplest form, the algorithm has a function, $Q$, that calculates the quality of any state-action combination
as follows: $$Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$$</p><h3 id=actions>Actions</h3><p>The <strong>action space</strong>, denoted $\mathcal{A}$, refers to the total number of possible actions a learner can take.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>
The cardinality (size) of $\mathcal{A}$ is determined by the number of actions to consider. In the context of a simulated market, we have 3 possible states: buy, sell, and exit market (going from a long/short position to holding 0 shares). When given some data, the Q-Learner will output a discrete value representing hold, buy, and sell: $$\mathcal{A} = \{0, 1, 2 \}$$</p><h3 id=states>States</h3><p>The <strong>state space</strong>, denoted $\mathcal{S}$, refers to all possible states of the learning environment at any given point in time. Determining the cardinality of the state space is tricky, as this number largely depends on our input features. Say, for example, we have 5 input features.</p><p>These input features must go through some function, $f(x)$, that turns the continuous data into a singular integer representing unique values for all of our features. The process of converting raw continuous data into a singular integer, regardless of the number of input features, is called <strong>discretization</strong>.</p><div style=text-align:center><img src="https://raw.githubusercontent.com/s-lasch/portfolio/refs/heads/master/exampleSite/content/blogs/ai-trader
/discretization.png" alt="Example of discretization"></div><p>The function $f(x)$ should also slice the input vector into a number of bins. Each bin is then mapped to an integer. In the image above, feature $X_1$ is put into bin 9, $X_2$ into bin 4, and $X_n$ into bin 2. These integers are then spliced together to create one integer, $249$, that represents this combination of inputs.</p><h3 id=reward-function>Reward Function</h3><p>Perhaps the most important pillar of a Q-Learner is the <strong>reward function</strong>, $R$. Of all the pillars, this should be most intuitive, especially for animal trainers. In this context, how do we reward our pets for making favorable decisions when training them to sit, or better yet, do a trick? Do we give them treats immediately after each success, or only once at the very end if they do everything right?</p><p>This decision will impact the time it will take for the learner to learn. Lots of questions need to be answered, mainly how do we reward our learner for making good trades, or penalize the learner for making subpar trades? What factors should be considered when making this decision? Do we base it on daily returns, or cumulative return for a prospective trade?</p><p>Q-Learners will converge to the optimal policy much faster when <strong>immediate reward</strong> is used.<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> In the context of pet training, immediately rewarding the animal after each success will help the animal to associate each action with positive reinforcement.</p><h2 id=conclusion>Conclusion</h2><p>In this post, we&rsquo;ve gone through an introduction to Q-Learning‚Äîthe three pillars, what they mean, and how we can apply this in a simulated market. Simply put, a Q-Learner uses the current <strong>state</strong> and tries out different <strong>actions</strong> to see what happens.</p><p>Based on the <strong>reward</strong> of an action, it learns which actions are the best to take in similar situations in the future. We also covered discretization, or the process of representing continuous data as discrete integers, and the importance of this in determining states.</p><h2 id=references>References</h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=http://incompleteideas.net/book/RLbook2020.pdf target=_blank rel=noopener>Reinforcement Learning: An Introduction 2nd Ed. (Ch. 1)</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://arxiv.org/pdf/2304.00026 target=_blank rel=noopener>Basic Q-learning to Proximal Policy Optimization (Chadi & Mousannif)</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class="row ps-3 pe-3"><div class="col-md-6 share-buttons"><strong>‡¶∂‡ßá‡ßü‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®:</strong>
<a class="btn icon-button bg-linkedin" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fslasch-portfolio.github.io%2fposts%2fai-trader%2fpart-1%2f&title=AI-Assisted%20Trader%20-%20Introduction%20%28Part%201%29" target=_blank><i class="fab fa-linkedin"></i>
</a><a class="btn icon-button" href="mailto:?subject=AI-Assisted%20Trader%20-%20Introduction%20%28Part%201%29&body=https%3a%2f%2fslasch-portfolio.github.io%2fposts%2fai-trader%2fpart-1%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/s-lasch/slasch-portfolio.github.io/edit/main/content/posts/ai-trader/part-1/index.md title="‡¶è‡¶á ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ‡¶ü‡¶ø ‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶ï‡¶∞‡ßÅ‡¶®" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
‡¶è‡¶á ‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ‡¶ü‡¶ø ‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶ï‡¶∞‡ßÅ‡¶®</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/introduction/ title=Introduction class="btn filled-button"><div><i class="fas fa-chevron-circle-left"></i> ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶¨‡¶∞‡ßç‡¶§‡ßÄ</div><div class=next-prev-text>Introduction</div></a></div><div class="col-md-6 next-article"><a href=/posts/ai-trader/part-2/ title="AI-Assisted Trader (Part 2)" class="btn filled-button"><div>‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡ßÄ <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>AI-Assisted Trader (Part 2)</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn type=button data-bs-toggle=tooltip data-bs-placement=left title="Scroll to top"><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center ps-3">‡¶∏‡ßÅ‡¶ö‡¶ø‡¶™‡¶§‡ßç‡¶∞</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#tldr>TLDR</a></li><li><a href=#q-learning>Q-Learning</a><ul><li><a href=#actions>Actions</a></li><li><a href=#states>States</a></li><li><a href=#reward-function>Reward Function</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-start"><div class="col-md-4 col-sm-12"><h5>‡¶®‡ßá‡¶≠‡¶ø‡¶ó‡ßá‡¶∂‡¶®</h5><ul><li class=nav-item><a class=smooth-scroll href=https://slasch-portfolio.github.io/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://slasch-portfolio.github.io/#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://slasch-portfolio.github.io/#experiences>Work Experience</a></li><li class=nav-item><a class=smooth-scroll href=https://slasch-portfolio.github.io/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://slasch-portfolio.github.io/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://slasch-portfolio.github.io/#publications>Publications</a></li><li class=nav-item><a class=smooth-scroll href=https://slasch-portfolio.github.io/#featured-posts>Featured Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Ø‡ßã‡¶ó‡¶æ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®‡¶É</h5><ul><li><a href=mailto:info@steven-lasch.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>info@steven-lasch.com</span></a></li><li><a href=https://github.com/s-lasch target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>s-lasch</span></a></li><li><a href=https://www.linkedin.com/in/steven-l-lasch target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>Steven Lasch</span></a></li><li><a href=https://www.researchgate.net/profile/Steven-Lasch target=_blank rel=noopener><span><i class="fab fa-researchgate"></i></span> <span>Steven Lasch</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-start"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu_be7279b2506ad3f1.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">¬© 2020 Copyright.</div><div class="col-md-4 text-end"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>‡¶™‡¶æ‡¶ì‡ßü‡¶æ‡¶∞‡¶° ‡¶¨‡¶æ‡¶á
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.fbb925f912f4010df78cab950f66761ded123027b3aaba43003a454c520135c0.js integrity="sha256-+7kl+RL0AQ33jKuVD2Z2He0SMCezqrpDADpFTFIBNcA=" defer></script></body></html>